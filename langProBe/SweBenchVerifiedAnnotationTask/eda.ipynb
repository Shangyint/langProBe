{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total 93 independent annotators\n",
      "nnotated 1699 testcases from SWE-bench\n",
      "Labels go from [0, 3], with 0 being no or minor issue, 3 being severe\n",
      "Difficulty annotated as \"How long will a developer take to solve\"\n",
      "Freeform text for \"other major issues\"\n",
      "\n",
      "Team of OpenAI engineers handlabeled 50 samples to high degree of confidence.\n",
      "Each annotator had to pass onboarding test.\n",
      "\n",
      "In the final dataset, each sample labeled 3 times by separate annotators\n",
      "Take the highest severeity label among 3 as the final label\n",
      "\n",
      "Annotation Criteria:\n",
      "1. Are the tasks well specified\n",
      "2. How valid are the evaluation criteria: Could the FAIL_TO_PASS tests fail even with a valid solution?\n",
      "3. (Not used for dataset filtering) How long will a developer take to solve the task?\n",
      "\n",
      "Final dataset: filter out any sample from the original test set where either task 1 or task 2 have ensemble \n",
      "label of 2 or above in severity\n",
      "\n",
      "Also filter out samples with other major issues flagged\n",
      "\n",
      "Include as many samples with difficulty 1-4 and >4 hours as possible, and then randomly sample the remainder to select 500 samples.\n",
      "\n",
      "Shown samples in the report include \"Problem Statement\", \"Are the tasks well specified\", \"FAIL_TO_PASS test (Only showing lines added during the original PR for brevity)\"\n"
     ]
    }
   ],
   "source": [
    "with open(\"ImportantNotesFromSweBenchVerifiedReport.md\", \"r\") as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_annotations = pd.read_csv('samples_with_3_annotations_public.csv')\n",
    "df_ensembled = pd.read_csv('ensembled_annotations_public.csv')\n",
    "df_swe_bench_full_test = pd.read_parquet('SWE-bench_hf_dataset_clone/data/test-00000-of-00001.parquet')\n",
    "df_swe_bench_full_dev = pd.read_parquet('SWE-bench_hf_dataset_clone/data/dev-00000-of-00001.parquet')\n",
    "df_swe_bench_full_train = pd.read_parquet('SWE-bench_hf_dataset_clone/data/train-00000-of-00001.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_swe_bench_full = pd.concat([df_swe_bench_full_test, df_swe_bench_full_dev, df_swe_bench_full_train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert df_all_annotations.shape[0] == df_ensembled.shape[0]*3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert (df_all_annotations['underspecified_problematic'] != (df_all_annotations['underspecified'] >= 2.0)).sum() == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert (df_all_annotations['false_negative_problematic'] != (df_all_annotations['false_negative'] >= 2.0)).sum() == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/19/0l8pzqh50_zb3f3_mdg6k5lm0000gp/T/ipykernel_2136/1658749138.py:10: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  instance_ids_to_filter_out = df_all_annotations.groupby('instance_id').apply(f).dropna().unique()\n"
     ]
    }
   ],
   "source": [
    "def f(xdf):\n",
    "    assert len(xdf) == 3\n",
    "    instance_id = xdf['instance_id'].iloc[0]\n",
    "    if xdf['problematic'].sum() > 0:\n",
    "        if xdf['underspecified_problematic'].sum() == 0 and xdf['false_negative_problematic'].sum() == 0:\n",
    "            assert xdf['other_major_issues'].sum() > 0\n",
    "            return instance_id\n",
    "    return None\n",
    "\n",
    "instance_ids_to_filter_out = df_all_annotations.groupby('instance_id').apply(f).dropna().unique()\n",
    "\n",
    "df_all_annotations_filtered = df_all_annotations[~df_all_annotations['instance_id'].isin(instance_ids_to_filter_out)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_swe_bench_full = df_swe_bench_full[df_swe_bench_full['instance_id'].isin(df_all_annotations_filtered['instance_id'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(row):\n",
    "    instance_id = row['instance_id']\n",
    "    row_swe_full = df_swe_bench_full[df_swe_bench_full['instance_id'] == instance_id].iloc[0]\n",
    "    return row_swe_full.drop(['instance_id', 'repo'])\n",
    "\n",
    "df_all_annotations_filtered = pd.concat([df_all_annotations_filtered, df_all_annotations_filtered.apply(f, axis=1)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    xdf = df_swe_bench_full[df_swe_bench_full['instance_id'] == x]\n",
    "    assert len(xdf) == 1, x\n",
    "    return xdf.iloc[0]['repo']\n",
    "\n",
    "df_all_annotations_filtered['repo'] = df_all_annotations_filtered['instance_id'].apply(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0              [astropy/wcs/wcsapi/tests/test_fitswcs.py]\n",
       "3                 [astropy/io/fits/tests/test_connect.py]\n",
       "4       [astropy/table/tests/conftest.py, astropy/tabl...\n",
       "5       [astropy/io/ascii/tests/test_ecsv.py, astropy/...\n",
       "6                   [astropy/io/ascii/tests/test_ecsv.py]\n",
       "                              ...                        \n",
       "2288                    [sympy/polys/tests/test_rings.py]\n",
       "2289                   [sympy/core/tests/test_numbers.py]\n",
       "2291           [sympy/parsing/tests/test_sympy_parser.py]\n",
       "2292     [sympy/stats/tests/test_matrix_distributions.py]\n",
       "2293         [sympy/physics/units/tests/test_prefixes.py]\n",
       "Length: 1689, dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "NON_TEST_EXTS = [\n",
    "    \".json\",\n",
    "    \".png\",\n",
    "    \"csv\",\n",
    "    \".txt\",\n",
    "    \".md\",\n",
    "    \".jpg\",\n",
    "    \".jpeg\",\n",
    "    \".pkl\",\n",
    "    \".yml\",\n",
    "    \".yaml\",\n",
    "    \".toml\",\n",
    "]\n",
    "\n",
    "def get_test_directives(instance) -> list:\n",
    "    \"\"\"\n",
    "    Get test directives from the test_patch of a task instance\n",
    "\n",
    "    Args:\n",
    "        instance (dict): task instance\n",
    "    Returns:\n",
    "        directives (list): List of test directives\n",
    "    \"\"\"\n",
    "    # For seq2seq code repos, testing command is fixed\n",
    "    if instance[\"repo\"] == \"swe-bench/humaneval\":\n",
    "        return [\"test.py\"]\n",
    "\n",
    "    # Get test directives from test patch and remove non-test files\n",
    "    diff_pat = r\"diff --git a/.* b/(.*)\"\n",
    "    test_patch = instance[\"test_patch\"]\n",
    "    directives = re.findall(diff_pat, test_patch)\n",
    "    directives = [\n",
    "        d for d in directives if not any(d.endswith(ext) for ext in NON_TEST_EXTS)\n",
    "    ]\n",
    "\n",
    "    # For Django tests, remove extension + \"tests/\" prefix and convert slashes to dots (module referencing)\n",
    "    if instance[\"repo\"] == \"django/django\":\n",
    "        directives_transformed = []\n",
    "        for d in directives:\n",
    "            d = d[: -len(\".py\")] if d.endswith(\".py\") else d\n",
    "            d = d[len(\"tests/\") :] if d.startswith(\"tests/\") else d\n",
    "            d = d.replace(\"/\", \".\")\n",
    "            directives_transformed.append(d)\n",
    "        directives = directives_transformed\n",
    "\n",
    "    return directives\n",
    "\n",
    "df_swe_bench_full.apply(get_test_directives, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DIFF_MODIFIED_FILE_REGEX = r\"--- a/(.*)\"\n",
    "\n",
    "# def f(test_patch):\n",
    "#     test_files = re.findall(DIFF_MODIFIED_FILE_REGEX, test_patch)\n",
    "#     print(test_files)\n",
    "\n",
    "# df_swe_bench_full['test_patch'].apply(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "repo\n",
      "astropy/astropy      71\n",
      "pylint-dev/pylint    37\n",
      "psf/requests         33\n",
      "mwaskom/seaborn       9\n",
      "pallets/flask         1\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "np.int64(151)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = 7\n",
    "print(df_swe_bench_full['repo'].value_counts().iloc[c:])\n",
    "df_swe_bench_full['repo'].value_counts().iloc[c:].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "repo\n",
       "django/django                646.0\n",
       "sympy/sympy                  298.0\n",
       "scikit-learn/scikit-learn    165.0\n",
       "sphinx-doc/sphinx            138.0\n",
       "matplotlib/matplotlib        125.0\n",
       "pytest-dev/pytest             88.0\n",
       "pydata/xarray                 78.0\n",
       "astropy/astropy               71.0\n",
       "pylint-dev/pylint             37.0\n",
       "psf/requests                  33.0\n",
       "mwaskom/seaborn                9.0\n",
       "pallets/flask                  1.0\n",
       "Name: count, dtype: float64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all_annotations_filtered['repo'].value_counts()/3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_for_test_from_each_repo_map = {\n",
    "    'django/django' : 36,\n",
    "    'sympy/sympy' : 36,\n",
    "    'scikit-learn/scikit-learn' : 36,\n",
    "    'sphinx-doc/sphinx' : 36,\n",
    "    'matplotlib/matplotlib' : 35,\n",
    "    'pytest-dev/pytest' : 35,\n",
    "    'pydata/xarray' : 35,\n",
    "    'astropy/astropy' : 71,\n",
    "    'pylint-dev/pylint' : 37,\n",
    "    'psf/requests' : 33,\n",
    "    'mwaskom/seaborn' : 9,\n",
    "    'pallets/flask' : 1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_or_val_instances = []\n",
    "test_instances = []\n",
    "for repo_name, num_test_samples in num_for_test_from_each_repo_map.items():\n",
    "    repo_instances = df_all_annotations_filtered[df_all_annotations_filtered['repo'] == repo_name]['instance_id'].unique()\n",
    "    test_instance_names = pd.Series(repo_instances).sample(n=num_test_samples, replace=False).tolist()\n",
    "    test_instances.extend(test_instance_names)\n",
    "    train_or_val_instances.extend(list(set(repo_instances) - set(test_instance_names)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1289, 400)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_or_val_instances), len(test_instances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1289, 400)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(train_or_val_instances)), len(set(test_instances))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_annotation_task_trainval_split = df_all_annotations_filtered[df_all_annotations_filtered['instance_id'].isin(train_or_val_instances)]\n",
    "df_annotation_task_test_split = df_all_annotations_filtered[df_all_annotations_filtered['instance_id'].isin(test_instances)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(df_annotation_task_trainval_split) == len(train_or_val_instances)*3\n",
    "assert len(df_annotation_task_test_split) == len(test_instances)*3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_annotation_task_trainval_split.to_csv('SweBenchVerifiedAnnotationTaskDataset/trainval_split.csv', index=False)\n",
    "df_annotation_task_test_split.to_csv('SweBenchVerifiedAnnotationTaskDataset/test_split.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langProBe",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
