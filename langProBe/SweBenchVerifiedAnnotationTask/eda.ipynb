{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total 93 independent annotators\n",
      "nnotated 1699 testcases from SWE-bench\n",
      "Labels go from [0, 3], with 0 being no or minor issue, 3 being severe\n",
      "Difficulty annotated as \"How long will a developer take to solve\"\n",
      "Freeform text for \"other major issues\"\n",
      "\n",
      "Team of OpenAI engineers handlabeled 50 samples to high degree of confidence.\n",
      "Each annotator had to pass onboarding test.\n",
      "\n",
      "In the final dataset, each sample labeled 3 times by separate annotators\n",
      "Take the highest severeity label among 3 as the final label\n",
      "\n",
      "Annotation Criteria:\n",
      "1. Are the tasks well specified\n",
      "2. How valid are the evaluation criteria: Could the FAIL_TO_PASS tests fail even with a valid solution?\n",
      "3. (Not used for dataset filtering) How long will a developer take to solve the task?\n",
      "\n",
      "Final dataset: filter out any sample from the original test set where either task 1 or task 2 have ensemble \n",
      "label of 2 or above in severity\n",
      "\n",
      "Also filter out samples with other major issues flagged\n",
      "\n",
      "Include as many samples with difficulty 1-4 and >4 hours as possible, and then randomly sample the remainder to select 500 samples.\n",
      "\n",
      "Shown samples in the report include \"Problem Statement\", \"Are the tasks well specified\", \"FAIL_TO_PASS test (Only showing lines added during the original PR for brevity)\"\n"
     ]
    }
   ],
   "source": [
    "with open(\"ImportantNotesFromSweBenchVerifiedReport.md\", \"r\") as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_annotations = pd.read_csv('samples_with_3_annotations_public.csv')\n",
    "df_ensembled = pd.read_csv('ensembled_annotations_public.csv')\n",
    "\n",
    "ds = load_dataset(\"princeton-nlp/SWE-bench\")\n",
    "df_swe_bench_full_test = ds['test'].data.to_pandas()\n",
    "df_swe_bench_full_dev = ds['dev'].data.to_pandas()\n",
    "df_swe_bench_full_train = ds['train'].data.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_swe_bench_full = pd.concat([df_swe_bench_full_test, df_swe_bench_full_dev, df_swe_bench_full_train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert df_all_annotations.shape[0] == df_ensembled.shape[0]*3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert (df_all_annotations['underspecified_problematic'] != (df_all_annotations['underspecified'] >= 2.0)).sum() == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert (df_all_annotations['false_negative_problematic'] != (df_all_annotations['false_negative'] >= 2.0)).sum() == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/19/0l8pzqh50_zb3f3_mdg6k5lm0000gp/T/ipykernel_2136/1658749138.py:10: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  instance_ids_to_filter_out = df_all_annotations.groupby('instance_id').apply(f).dropna().unique()\n"
     ]
    }
   ],
   "source": [
    "def f(xdf):\n",
    "    assert len(xdf) == 3\n",
    "    instance_id = xdf['instance_id'].iloc[0]\n",
    "    if xdf['problematic'].sum() > 0:\n",
    "        if xdf['underspecified_problematic'].sum() == 0 and xdf['false_negative_problematic'].sum() == 0:\n",
    "            assert xdf['other_major_issues'].sum() > 0\n",
    "            return instance_id\n",
    "    return None\n",
    "\n",
    "instance_ids_to_filter_out = df_all_annotations.groupby('instance_id').apply(f).dropna().unique()\n",
    "\n",
    "df_all_annotations_filtered = df_all_annotations[~df_all_annotations['instance_id'].isin(instance_ids_to_filter_out)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_swe_bench_full = df_swe_bench_full[df_swe_bench_full['instance_id'].isin(df_all_annotations_filtered['instance_id'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(row):\n",
    "    instance_id = row['instance_id']\n",
    "    row_swe_full = df_swe_bench_full[df_swe_bench_full['instance_id'] == instance_id].iloc[0]\n",
    "    return row_swe_full.drop(['instance_id', 'repo'])\n",
    "\n",
    "df_all_annotations_filtered = pd.concat([df_all_annotations_filtered, df_all_annotations_filtered.apply(f, axis=1)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    xdf = df_swe_bench_full[df_swe_bench_full['instance_id'] == x]\n",
    "    assert len(xdf) == 1, x\n",
    "    return xdf.iloc[0]['repo']\n",
    "\n",
    "df_all_annotations_filtered['repo'] = df_all_annotations_filtered['instance_id'].apply(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0              [astropy/wcs/wcsapi/tests/test_fitswcs.py]\n",
       "3                 [astropy/io/fits/tests/test_connect.py]\n",
       "4       [astropy/table/tests/conftest.py, astropy/tabl...\n",
       "5       [astropy/io/ascii/tests/test_ecsv.py, astropy/...\n",
       "6                   [astropy/io/ascii/tests/test_ecsv.py]\n",
       "                              ...                        \n",
       "2288                    [sympy/polys/tests/test_rings.py]\n",
       "2289                   [sympy/core/tests/test_numbers.py]\n",
       "2291           [sympy/parsing/tests/test_sympy_parser.py]\n",
       "2292     [sympy/stats/tests/test_matrix_distributions.py]\n",
       "2293         [sympy/physics/units/tests/test_prefixes.py]\n",
       "Length: 1689, dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "NON_TEST_EXTS = [\n",
    "    \".json\",\n",
    "    \".png\",\n",
    "    \"csv\",\n",
    "    \".txt\",\n",
    "    \".md\",\n",
    "    \".jpg\",\n",
    "    \".jpeg\",\n",
    "    \".pkl\",\n",
    "    \".yml\",\n",
    "    \".yaml\",\n",
    "    \".toml\",\n",
    "]\n",
    "\n",
    "def get_test_directives(instance) -> list:\n",
    "    \"\"\"\n",
    "    Get test directives from the test_patch of a task instance\n",
    "\n",
    "    Args:\n",
    "        instance (dict): task instance\n",
    "    Returns:\n",
    "        directives (list): List of test directives\n",
    "    \"\"\"\n",
    "    # For seq2seq code repos, testing command is fixed\n",
    "    if instance[\"repo\"] == \"swe-bench/humaneval\":\n",
    "        return [\"test.py\"]\n",
    "\n",
    "    # Get test directives from test patch and remove non-test files\n",
    "    diff_pat = r\"diff --git a/.* b/(.*)\"\n",
    "    test_patch = instance[\"test_patch\"]\n",
    "    directives = re.findall(diff_pat, test_patch)\n",
    "    directives = [\n",
    "        d for d in directives if not any(d.endswith(ext) for ext in NON_TEST_EXTS)\n",
    "    ]\n",
    "\n",
    "    # For Django tests, remove extension + \"tests/\" prefix and convert slashes to dots (module referencing)\n",
    "    if instance[\"repo\"] == \"django/django\":\n",
    "        directives_transformed = []\n",
    "        for d in directives:\n",
    "            d = d[: -len(\".py\")] if d.endswith(\".py\") else d\n",
    "            d = d[len(\"tests/\") :] if d.startswith(\"tests/\") else d\n",
    "            d = d.replace(\"/\", \".\")\n",
    "            directives_transformed.append(d)\n",
    "        directives = directives_transformed\n",
    "\n",
    "    return directives\n",
    "\n",
    "df_swe_bench_full.apply(get_test_directives, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DIFF_MODIFIED_FILE_REGEX = r\"--- a/(.*)\"\n",
    "\n",
    "# def f(test_patch):\n",
    "#     test_files = re.findall(DIFF_MODIFIED_FILE_REGEX, test_patch)\n",
    "#     print(test_files)\n",
    "\n",
    "# df_swe_bench_full['test_patch'].apply(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "repo\n",
      "astropy/astropy      71\n",
      "pylint-dev/pylint    37\n",
      "psf/requests         33\n",
      "mwaskom/seaborn       9\n",
      "pallets/flask         1\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "np.int64(151)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = 7\n",
    "print(df_swe_bench_full['repo'].value_counts().iloc[c:])\n",
    "df_swe_bench_full['repo'].value_counts().iloc[c:].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "repo\n",
       "django/django                646.0\n",
       "sympy/sympy                  298.0\n",
       "scikit-learn/scikit-learn    165.0\n",
       "sphinx-doc/sphinx            138.0\n",
       "matplotlib/matplotlib        125.0\n",
       "pytest-dev/pytest             88.0\n",
       "pydata/xarray                 78.0\n",
       "astropy/astropy               71.0\n",
       "pylint-dev/pylint             37.0\n",
       "psf/requests                  33.0\n",
       "mwaskom/seaborn                9.0\n",
       "pallets/flask                  1.0\n",
       "Name: count, dtype: float64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all_annotations_filtered['repo'].value_counts()/3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_for_test_from_each_repo_map = {\n",
    "    'django/django' : 36,\n",
    "    'sympy/sympy' : 36,\n",
    "    'scikit-learn/scikit-learn' : 36,\n",
    "    'sphinx-doc/sphinx' : 36,\n",
    "    'matplotlib/matplotlib' : 35,\n",
    "    'pytest-dev/pytest' : 35,\n",
    "    'pydata/xarray' : 35,\n",
    "    'astropy/astropy' : 71,\n",
    "    'pylint-dev/pylint' : 37,\n",
    "    'psf/requests' : 33,\n",
    "    'mwaskom/seaborn' : 9,\n",
    "    'pallets/flask' : 1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_or_val_instances = []\n",
    "test_instances = []\n",
    "for repo_name, num_test_samples in num_for_test_from_each_repo_map.items():\n",
    "    repo_instances = df_all_annotations_filtered[df_all_annotations_filtered['repo'] == repo_name]['instance_id'].unique()\n",
    "    test_instance_names = pd.Series(repo_instances).sample(n=num_test_samples, replace=False).tolist()\n",
    "    test_instances.extend(test_instance_names)\n",
    "    train_or_val_instances.extend(list(set(repo_instances) - set(test_instance_names)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1289, 400)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_or_val_instances), len(test_instances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1289, 400)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(train_or_val_instances)), len(set(test_instances))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_annotation_task_trainval_split = df_all_annotations_filtered[df_all_annotations_filtered['instance_id'].isin(train_or_val_instances)]\n",
    "df_annotation_task_test_split = df_all_annotations_filtered[df_all_annotations_filtered['instance_id'].isin(test_instances)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(df_annotation_task_trainval_split) == len(train_or_val_instances)*3\n",
    "assert len(df_annotation_task_test_split) == len(test_instances)*3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_annotation_task_trainval_split.to_csv('SweBenchVerifiedAnnotationTaskDataset/trainval_split.csv', index=False)\n",
    "df_annotation_task_test_split.to_csv('SweBenchVerifiedAnnotationTaskDataset/test_split.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_annotation_task_test_split = pd.read_csv('SweBenchVerifiedAnnotationTaskDataset/test_split.csv')\n",
    "df_annotation_task_trainval_split = pd.read_csv('SweBenchVerifiedAnnotationTaskDataset/trainval_split.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(dfx, keyname):\n",
    "    idx_to_score_map = {}\n",
    "    for idx, row in dfx[dfx[keyname].isna()].iterrows():\n",
    "        instance_id = row['instance_id']\n",
    "        other_rows = dfx[dfx['instance_id'] == instance_id]\n",
    "        for _, other_row in other_rows.iterrows():\n",
    "            print(other_row[keyname])\n",
    "        print('notes', row[keyname + '_notes'])\n",
    "        print('problematic', row[keyname + '_problematic'])\n",
    "        print(instance_id)\n",
    "        print(\"userid\", row['user_id'])\n",
    "        print(\"\")\n",
    "        input_score = float(input())\n",
    "        \n",
    "        idx_to_score_map[idx] = input_score\n",
    "\n",
    "    for idx, score in idx_to_score_map.items():\n",
    "        dfx.loc[idx, keyname] = score\n",
    "    \n",
    "    return dfx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "nan\n",
      "2.0\n",
      "notes When running Django's `runserver` with the `--nothreading` option, it may stop responding because web browsers like Chrome and Firefox use multiple connections with the \"Connection: keep-alive\" header by default. The browser keeps the first connection open for a long time, causing a delay or failure in handling the next connection. Although issue is clearly specified but its not specified what is expected from this. Either it can be closed from the headers or keep alive can be removed but no approach is preferred or no expectations are set hence due to this ambiguity its rated 2.\n",
      "problematic False\n",
      "django__django-11543\n",
      "userid 30\n",
      "\n",
      "0.0\n",
      "0.0\n",
      "nan\n",
      "notes The ticket is clear. It seems that the POSIX implementations for `fcntl` function is misinterpreting the return value of the function and for that returns to the function user an invalid results. For that, a simple fix needs to be done and it is very clear from the ticket description. \n",
      "problematic False\n",
      "django__django-13410\n",
      "userid 7\n",
      "\n",
      "2.0\n",
      "nan\n",
      "1.0\n",
      "notes `SelectMultiple` in `ModelAdminForm` display help text when `allow_multiple_selected` is `False`. This happens because the help text rendering logic only checks if the widget is an instance of `SelectMultiple` and not whether `allow_multiple_selected` is `True`. This behaviour is not expected and the condition needs to be added. Since everything is clear, its rated 0. \n",
      "problematic False\n",
      "django__django-15799\n",
      "userid 30\n",
      "\n",
      "1.0\n",
      "1.0\n",
      "nan\n",
      "notes The issue is very clear. The function `frac(zoo)` gives `TypeError` when it shouldn't. However, there is a blank to fill which is what the expected output should be. The user seems not sure what the expected behavior should be. \n",
      "problematic False\n",
      "sympy__sympy-17271\n",
      "userid 7\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_annotation_task_trainval_split = f(df_annotation_task_trainval_split, 'underspecified')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_annotation_task_trainval_split.to_csv('SweBenchVerifiedAnnotationTaskDataset/trainval_split.csv', index=False)\n",
    "df_annotation_task_test_split.to_csv('SweBenchVerifiedAnnotationTaskDataset/test_split.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_annotation_task_test_split = pd.read_csv('SweBenchVerifiedAnnotationTaskDataset/test_split.csv')\n",
    "df_annotation_task_trainval_split = pd.read_csv('SweBenchVerifiedAnnotationTaskDataset/trainval_split.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "1.0\n",
      "nan\n",
      "notes The tests cover exactly the behaviour described in the problem statement and any correct solution should pass the tests, namely checking the migrations on the ModelStates created, 'contract', 'authors', 'testapp'.\n",
      "So we can pick label 0 for this one.\n",
      "problematic False\n",
      "django__django-15973\n",
      "userid 53\n",
      "\n",
      "1.0\n",
      "nan\n",
      "2.0\n",
      "notes The test seems to incorporate tool._selection_artist, tool.set_handle_props and tool.set_props as a fix to the issue mentioned. These tests work but there is a chance that other function name may have been adopted or just some selected function name among these three. Infact we had a different name tool._corner_handles.artist() that seems to address the solution too. This would have led to a perfectly reasonable solutions missed by the tests too. Moreso, there was suggestion for detailed depreciation warning in the issue description that wasn't handled by the test.\n",
      "problematic False\n",
      "matplotlib__matplotlib-20693\n",
      "userid 16\n",
      "\n",
      "nan\n",
      "2.0\n",
      "3.0\n",
      "notes While the test patch seems to address the issue related to check_class_weight_balanced_linear_classifier when it has has bad weight balance and raise an exception. This test patch would have partially worked (bad weight balance is just one of numerous way to test that the check_class_weight_balanced_linear_classifier works) but the fact that the issue is vague, without any consultation one won't be able to come up with the solution as that. Moreso, the test patch didn't address check_class_weight_balanced_classifiers. Thus, this is rated 3.\n",
      "problematic False\n",
      "scikit-learn__scikit-learn-13313\n",
      "userid 16\n",
      "\n",
      "nan\n",
      "0.0\n",
      "0.0\n",
      "notes The test patch lacks verifications for github link checks, which could cause a valid solution to fail.\n",
      "problematic False\n",
      "sphinx-doc__sphinx-9467\n",
      "userid 9\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_annotation_task_trainval_split = f(df_annotation_task_trainval_split, 'false_negative')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_annotation_task_trainval_split.to_csv('SweBenchVerifiedAnnotationTaskDataset/trainval_split.csv', index=False)\n",
    "df_annotation_task_test_split.to_csv('SweBenchVerifiedAnnotationTaskDataset/test_split.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_annotation_task_test_split = f(df_annotation_task_test_split, 'underspecified')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "nan\n",
      "3.0\n",
      "notes The test patch adds three different functions to verify the linspace (with and without steps provided) and specific date/time formats. \n",
      "With this verification, any valid solution will pass the test.\n",
      "problematic False\n",
      "astropy__astropy-13132\n",
      "userid 9\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_annotation_task_test_split = f(df_annotation_task_test_split, 'false_negative')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_annotation_task_trainval_split.to_csv('SweBenchVerifiedAnnotationTaskDataset/trainval_split.csv', index=False)\n",
    "df_annotation_task_test_split.to_csv('SweBenchVerifiedAnnotationTaskDataset/test_split.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langProBe",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
